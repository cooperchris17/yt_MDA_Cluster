{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows a comparison of different machine learning methods\n",
    "- the input features were linguistic features, Random Forest Classifier was the most accurate\n",
    "\n",
    "##### Bootstrapping \n",
    "- with 500 texts in each class in the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cefr</th>\n",
       "      <th>Dep_Clauses_per_Clause</th>\n",
       "      <th>lexical_density_types</th>\n",
       "      <th>B1</th>\n",
       "      <th>wpm</th>\n",
       "      <th>B2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1_a_good_nights_sleep</td>\n",
       "      <td>A1</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.728972</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>110.973451</td>\n",
       "      <td>0.483092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1_a_request_from_your_boss</td>\n",
       "      <td>A1</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>1.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1_a_voicemail_message</td>\n",
       "      <td>A1</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.709220</td>\n",
       "      <td>103.235294</td>\n",
       "      <td>2.127660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1_A2_episode_01_they_meet</td>\n",
       "      <td>A1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>49.390244</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1_A2_episode_02_toms_party</td>\n",
       "      <td>A1</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>57.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>C2Prof_16-20</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.793210</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>201.115880</td>\n",
       "      <td>1.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>C2Prof_21-30</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>4.362416</td>\n",
       "      <td>218.282209</td>\n",
       "      <td>1.845638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>C2Prof_3-4</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>182.686567</td>\n",
       "      <td>0.980392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>C2Prof_5-6</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>3.389831</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>1.694915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>C2Prof_7-15</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.785455</td>\n",
       "      <td>7.885906</td>\n",
       "      <td>162.818182</td>\n",
       "      <td>2.684564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename cefr  Dep_Clauses_per_Clause  \\\n",
       "0         A1_a_good_nights_sleep   A1                0.233333   \n",
       "1    A1_a_request_from_your_boss   A1                0.076923   \n",
       "2         A1_a_voicemail_message   A1                0.187500   \n",
       "3     A1_A2_episode_01_they_meet   A1                0.000000   \n",
       "4    A1_A2_episode_02_toms_party   A1                0.035714   \n",
       "..                           ...  ...                     ...   \n",
       "723                 C2Prof_16-20   B2                0.378947   \n",
       "724                 C2Prof_21-30   B2                0.454545   \n",
       "725                   C2Prof_3-4   B2                0.266667   \n",
       "726                   C2Prof_5-6   B2                0.434783   \n",
       "727                  C2Prof_7-15   B2                0.403509   \n",
       "\n",
       "     lexical_density_types        B1         wpm        B2  \n",
       "0                 0.728972  4.347826  110.973451  0.483092  \n",
       "1                 0.642857  2.352941  135.000000  1.176471  \n",
       "2                 0.666667  0.709220  103.235294  2.127660  \n",
       "3                 0.614035  0.769231   49.390244  0.000000  \n",
       "4                 0.539683  3.225806   57.200000  0.000000  \n",
       "..                     ...       ...         ...       ...  \n",
       "723               0.793210  7.692308  201.115880  1.923077  \n",
       "724               0.772549  4.362416  218.282209  1.845638  \n",
       "725               0.660377  5.882353  182.686567  0.980392  \n",
       "726               0.660194  3.389831  178.000000  1.694915  \n",
       "727               0.785455  7.885906  162.818182  2.684564  \n",
       "\n",
       "[728 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# load data\n",
    "datafile_path = \"bc_cam_5_taales_etc_variables.csv\"\n",
    "\n",
    "df = pd.read_csv(datafile_path)\n",
    "\n",
    "df['cefr'] = df['filename'].str[:2]\n",
    "# convert all C level labels to B2\n",
    "df['cefr'] = df['cefr'].replace({'C1': 'B2', 'C2': 'B2'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data\n",
    "- the scales are all different for the TAALES, etc variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cefr</th>\n",
       "      <th>Dep_Clauses_per_Clause</th>\n",
       "      <th>lexical_density_types</th>\n",
       "      <th>B1</th>\n",
       "      <th>wpm</th>\n",
       "      <th>B2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1_a_good_nights_sleep</td>\n",
       "      <td>A1</td>\n",
       "      <td>-0.225103</td>\n",
       "      <td>0.322803</td>\n",
       "      <td>0.066933</td>\n",
       "      <td>-0.855838</td>\n",
       "      <td>-0.854962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1_a_request_from_your_boss</td>\n",
       "      <td>A1</td>\n",
       "      <td>-1.378203</td>\n",
       "      <td>-1.038086</td>\n",
       "      <td>-0.768515</td>\n",
       "      <td>-0.220806</td>\n",
       "      <td>-0.307815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1_a_voicemail_message</td>\n",
       "      <td>A1</td>\n",
       "      <td>-0.562999</td>\n",
       "      <td>-0.661819</td>\n",
       "      <td>-1.456898</td>\n",
       "      <td>-1.060361</td>\n",
       "      <td>0.442769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1_A2_episode_01_they_meet</td>\n",
       "      <td>A1</td>\n",
       "      <td>-1.945302</td>\n",
       "      <td>-1.493566</td>\n",
       "      <td>-1.431766</td>\n",
       "      <td>-2.483510</td>\n",
       "      <td>-1.236170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1_A2_episode_02_toms_party</td>\n",
       "      <td>A1</td>\n",
       "      <td>-1.682006</td>\n",
       "      <td>-2.668574</td>\n",
       "      <td>-0.402963</td>\n",
       "      <td>-2.277094</td>\n",
       "      <td>-1.236170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>C2Prof_16-20</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.848404</td>\n",
       "      <td>1.337967</td>\n",
       "      <td>1.467586</td>\n",
       "      <td>1.526667</td>\n",
       "      <td>0.281333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>C2Prof_21-30</td>\n",
       "      <td>B2</td>\n",
       "      <td>1.405735</td>\n",
       "      <td>1.011459</td>\n",
       "      <td>0.073043</td>\n",
       "      <td>1.980380</td>\n",
       "      <td>0.220225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>C2Prof_3-4</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>-0.761211</td>\n",
       "      <td>0.709586</td>\n",
       "      <td>1.039572</td>\n",
       "      <td>-0.462541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>C2Prof_5-6</td>\n",
       "      <td>B2</td>\n",
       "      <td>1.260038</td>\n",
       "      <td>-0.764105</td>\n",
       "      <td>-0.334271</td>\n",
       "      <td>0.915704</td>\n",
       "      <td>0.101290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>C2Prof_7-15</td>\n",
       "      <td>B2</td>\n",
       "      <td>1.029478</td>\n",
       "      <td>1.215408</td>\n",
       "      <td>1.548664</td>\n",
       "      <td>0.514441</td>\n",
       "      <td>0.882223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename cefr  Dep_Clauses_per_Clause  \\\n",
       "0         A1_a_good_nights_sleep   A1               -0.225103   \n",
       "1    A1_a_request_from_your_boss   A1               -1.378203   \n",
       "2         A1_a_voicemail_message   A1               -0.562999   \n",
       "3     A1_A2_episode_01_they_meet   A1               -1.945302   \n",
       "4    A1_A2_episode_02_toms_party   A1               -1.682006   \n",
       "..                           ...  ...                     ...   \n",
       "723                 C2Prof_16-20   B2                0.848404   \n",
       "724                 C2Prof_21-30   B2                1.405735   \n",
       "725                   C2Prof_3-4   B2                0.020640   \n",
       "726                   C2Prof_5-6   B2                1.260038   \n",
       "727                  C2Prof_7-15   B2                1.029478   \n",
       "\n",
       "     lexical_density_types        B1       wpm        B2  \n",
       "0                 0.322803  0.066933 -0.855838 -0.854962  \n",
       "1                -1.038086 -0.768515 -0.220806 -0.307815  \n",
       "2                -0.661819 -1.456898 -1.060361  0.442769  \n",
       "3                -1.493566 -1.431766 -2.483510 -1.236170  \n",
       "4                -2.668574 -0.402963 -2.277094 -1.236170  \n",
       "..                     ...       ...       ...       ...  \n",
       "723               1.337967  1.467586  1.526667  0.281333  \n",
       "724               1.011459  0.073043  1.980380  0.220225  \n",
       "725              -0.761211  0.709586  1.039572 -0.462541  \n",
       "726              -0.764105 -0.334271  0.915704  0.101290  \n",
       "727               1.215408  1.548664  0.514441  0.882223  \n",
       "\n",
       "[728 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns to normalize\n",
    "columns_to_normalize = ['Dep_Clauses_per_Clause', 'lexical_density_types', 'B1', 'wpm', 'B2']\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the columns\n",
    "df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adapted the code, so the texts in the training set could be bootstrapped (using resample in sklearn)\n",
    "- maybe this code is not DRY (I think there are 1 or 2 uneccesary steps and it could be cleaner, but it works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['cefr', 'filename'], axis=1), df['cefr'], test_size=0.2, random_state=160923\n",
    ")\n",
    "\n",
    "# Convert the training set lists to a DataFrame\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the classes in the training set\n",
    "class_A1 = df_train[df_train['cefr'] == \"A1\"]\n",
    "class_A2 = df_train[df_train['cefr'] == \"A2\"]\n",
    "class_B1 = df_train[df_train['cefr'] == \"B1\"]\n",
    "class_B2 = df_train[df_train['cefr'] == \"B2\"]\n",
    "\n",
    "# Bootstrap each class in the training set to have 500 samples\n",
    "class_A1_sampled = resample(class_A1, replace=True, n_samples=500, random_state=160923)\n",
    "class_A2_sampled = resample(class_A2, replace=True, n_samples=500, random_state=160923)\n",
    "class_B1_sampled = resample(class_B1, replace=True, n_samples=500, random_state=160923)\n",
    "class_B2_sampled = resample(class_B2, replace=True, n_samples=500, random_state=160923)\n",
    "\n",
    "# Concatenate the bootstrapped classes back together\n",
    "df_train_sampled = pd.concat([class_A1_sampled, class_A2_sampled, class_B1_sampled, class_B2_sampled])\n",
    "\n",
    "# Now can use df_train_sampled for machine learning tasks\n",
    "X_train_sampled = df_train_sampled.drop('cefr', axis=1)\n",
    "y_train_sampled = df_train_sampled.cefr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier\n",
    "\n",
    "##### This is the best performing for linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.60      0.75      0.67         8\n",
      "          A2       0.64      0.69      0.67        13\n",
      "          B1       0.54      0.57      0.56        44\n",
      "          B2       0.78      0.73      0.75        81\n",
      "\n",
      "    accuracy                           0.68       146\n",
      "   macro avg       0.64      0.68      0.66       146\n",
      "weighted avg       0.68      0.68      0.68       146\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 6  2  0  0]\n",
      " [ 2  9  2  0]\n",
      " [ 1  1 25 17]\n",
      " [ 1  2 19 59]]\n"
     ]
    }
   ],
   "source": [
    "# train random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train_sampled, y_train_sampled)\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)\n",
    "\n",
    "# print confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.56      0.62      0.59         8\n",
      "          A2       0.39      0.69      0.50        13\n",
      "          B1       0.51      0.55      0.53        44\n",
      "          B2       0.82      0.68      0.74        81\n",
      "\n",
      "    accuracy                           0.64       146\n",
      "   macro avg       0.57      0.64      0.59       146\n",
      "weighted avg       0.67      0.64      0.65       146\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 5  3  0  0]\n",
      " [ 2  9  2  0]\n",
      " [ 1  7 24 12]\n",
      " [ 1  4 21 55]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# train SVM classifier\n",
    "clf = svm.SVC(probability=True)\n",
    "clf.fit(X_train_sampled, y_train_sampled)\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)\n",
    "\n",
    "# print confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN): This is a type of instance-based learning that classifies a data point based on how its neighbors are classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.33      0.25      0.29         8\n",
      "          A2       0.42      0.62      0.50        13\n",
      "          B1       0.40      0.48      0.44        44\n",
      "          B2       0.70      0.59      0.64        81\n",
      "\n",
      "    accuracy                           0.54       146\n",
      "   macro avg       0.46      0.48      0.47       146\n",
      "weighted avg       0.56      0.54      0.55       146\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2  6  0  0]\n",
      " [ 2  8  2  1]\n",
      " [ 1  2 21 20]\n",
      " [ 1  3 29 48]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "clf.fit(X_train_sampled, y_train_sampled)\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)\n",
    "\n",
    "# print confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks: Deep learning models, especially neural networks, can be very effective on tasks with large amounts of data and many input features. Scikit-learn provides simple neural networks models through the MLPClassifier class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.75      0.75      0.75         8\n",
      "          A2       0.38      0.69      0.49        13\n",
      "          B1       0.52      0.50      0.51        44\n",
      "          B2       0.83      0.74      0.78        81\n",
      "\n",
      "    accuracy                           0.66       146\n",
      "   macro avg       0.62      0.67      0.63       146\n",
      "weighted avg       0.69      0.66      0.67       146\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 6  2  0  0]\n",
      " [ 2  9  2  0]\n",
      " [ 0 10 22 12]\n",
      " [ 0  3 18 60]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coope\\Documents\\PHD\\Text_Classification\\BERT_etc\\cefr_llms\\venv-cefr\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10, 10, 10))\n",
    "\n",
    "clf.fit(X_train_sampled, y_train_sampled)\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)\n",
    "\n",
    "# print confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cefr-venv",
   "language": "python",
   "name": "cefr-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
